import numpy as np
import pandas as pd                 #opend and read csv files
import csv                          #save predict csv file
import math
import matplotlib.pyplot as plt
import os

if __name__ == '__main__':
    data = pd.read_csv('../data/train.csv', encoding='big5')
    data = data.iloc[:, 3:]
    data[data=='NR'] = 0

    raw_data = data.to_numpy()

    month_data = {}
    for month in range(12):
        sample = np.empty([18, 480])
        for day in range(20):
            sample[:, day*24:(day+1)*24] = raw_data[18*(20*month+day):18*(20*month+day+1), :]

        month_data[month] = sample

    x = np.empty([12*471, 18*9], dtype=float)
    y = np.empty([12*471, 1], dtype=float)
    for month in range(12):
        for day in range(20):
            for hour in range(24):

                if day==19 and hour>14:
                    continue

                x[month*471+day*24+hour, :] = month_data[month][:, 0+hour+day*24:9+hour+day*24].reshape(1, -1)
                y[month*471+day*24+hour, :] = month_data[month][9, day*24+hour+9]

    mean_x = np.mean(x, axis = 0)
    std_x = np.std(x, axis=0)

    for i in range(len(x)):
        for j in range(len(x[0])):
            if std_x[j]!=0:
                x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]

    dim = 18*9 + 1
    w = np.zeros([dim, 1])
    x = np.concatenate((np.ones([12*471, 1]), x), axis=1).astype(float)
    x_train_set = x[:math.floor(len(x) * 0.8), :]
    y_train_set = y[:math.floor(len(y) * 0.8), :]
    x_validation = x[math.floor(len(x) * 0.8):, :]
    y_validation = y[math.floor(len(x) * 0.8):, :]
    learning_rate = 0.0005
    iter_time = 10000
    adagrad = np.zeros([dim, 1])
    eps = 1e-10
    loss_list = []
    acc_list = []
    for t in range(iter_time):
        loss = np.sqrt(np.sum(np.power((np.dot(x_train_set,w) - y_train_set), 2)) / (len(x_train_set)))
        loss_list.append(loss)
        if(t % 10 == 0):
            print(str(t) + ':' + str(loss))
        gradient = 2*np.dot(x_train_set.transpose(), np.dot(x_train_set, w)-y_train_set)

        adagrad += gradient**2

        w = w - learning_rate*gradient / np.sqrt(adagrad+eps)

        if(t % 50 ==0):
            y_hat = np.dot(x_validation, w)
            acc = y_hat - y_validation
            temp = 0
            for num in range(len(acc)):
                temp = (acc[num]+temp)/len(acc)
            acc_list.append(temp)

    print(loss_list)
    loss_list_end = np.array(loss_list)
    acc_list_end = np.array(acc_list)

    fig1 = plt.figure(figsize=(8,4))
    plt.plot(range(iter_time), loss_list_end, 'o-', label='loss value')
    plt.xlabel('iter_num')
    plt.ylabel('loss value')
    plt.title('loss value at every iteration')
    plt.legend(loc='best')

    left, bottom, width, height = 0.53, 0.3, 0.35, 0.4
    subfig = fig1.add_axes([left, bottom, width, height])
    subfig.plot(range(iter_time-25, iter_time), loss_list[len(loss_list)-25:])
    subfig.set_title('sug img')

    if not os.path.exists('./results'):
        os.mkdir('./results')

    plt.savefig('./results/compare_loss.png')
    plt.show()

    print(len(acc_list_end), acc_list_end.shape)
    fig2 = plt.figure(figsize=(8,4))
    plt.plot(range(len(acc_list_end)), acc_list_end, 'o-', label='acc value')
    plt.xlabel('num')
    plt.ylabel('acc value')
    plt.title('acc value at per-50')
    plt.legend(loc='best')

    plt.savefig('./results/compare_acc.png')
    plt.show()

    np.save('./results/bias_and_weight.npy', w)
    print(w)
